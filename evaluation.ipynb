{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n",
      "{}\n",
      "()\n",
      "{}\n",
      "()\n",
      "{}\n",
      "()\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "# %load  evaluation.py\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, average_precision_score, f1_score, roc_auc_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from keras.models import Sequential\n",
    "import time\n",
    "from sklearn.model_selection import KFold,train_test_split,cross_val_score\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "from utils import *\n",
    "import keras\n",
    "from keras_applications.densenet import layers\n",
    "from utils import *\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def calculate_performace(test_num, pred_y, labels):\n",
    "    '''\n",
    "    Evaluate algorithm performance.\n",
    "    '''\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    tn = 0\n",
    "    fn = 0\n",
    "    for index in range(test_num):\n",
    "        if labels[index] == 1:\n",
    "            if labels[index] == pred_y[index]:\n",
    "                tp = tp + 1\n",
    "            else:\n",
    "                fn = fn + 1\n",
    "        else:\n",
    "            if labels[index] == pred_y[index]:\n",
    "                tn = tn + 1\n",
    "            else:\n",
    "                fp = fp + 1\n",
    "    accuracy = float(tp + tn) / test_num\n",
    "    precision = float(tp) / (tp + fp + 1e-06)\n",
    "    sensitivity = float(tp) / (tp + fn + 1e-06)\n",
    "    recall = float(tp) / (tp + fn + 1e-06)\n",
    "    specificity = float(tn) / (tn + fp + 1e-06)\n",
    "    f1_score = float(2 * tp) / (2 * tp + fp + fn + 1e-06)\n",
    "    MCC = float(tp * tn - fp * fn) / (np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)))\n",
    "    return  accuracy, precision, sensitivity, recall, specificity, MCC, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PPIPrediction(embedding_look_up, original_graph, train_graph,G0, test_pos_edges, seed,training_pos_edges):\n",
    "    random.seed(seed)\n",
    "    train_neg_edges = generate_neg_edges(G0, len(training_pos_edges), seed)\n",
    "    G_aux = copy.deepcopy(G0)\n",
    "    # create a auxiliary graph to ensure that testing negative edges will not used in training\n",
    "    for edge in train_neg_edges:\n",
    "        node1 = edge[0]\n",
    "        node2 = edge[1]\n",
    "        G_aux.remove_edge(node1,node2)\n",
    "    test_neg_edges = G_aux.edges()\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    for edge in training_pos_edges:\n",
    "        node_u_emb = embedding_look_up[edge[0]]\n",
    "        node_v_emb = embedding_look_up[edge[1]]\n",
    "        feature_vector = np.append(node_u_emb, node_v_emb)\n",
    "        X_train.append(feature_vector)\n",
    "        y_train.append(1)\n",
    "    for edge in train_neg_edges:\n",
    "        node_u_emb = embedding_look_up[edge[0]]\n",
    "        node_v_emb = embedding_look_up[edge[1]]\n",
    "        feature_vector = np.append(node_u_emb, node_v_emb)\n",
    "        X_train.append(feature_vector)\n",
    "        y_train.append(0)\n",
    "\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    for edge in test_pos_edges:\n",
    "        node_u_emb = embedding_look_up[edge[0]]\n",
    "        node_v_emb = embedding_look_up[edge[1]]\n",
    "        feature_vector = np.append(node_u_emb, node_v_emb)\n",
    "        X_test.append(feature_vector)\n",
    "        y_test.append(1)\n",
    "    for edge in test_neg_edges:\n",
    "        node_u_emb = embedding_look_up[edge[0]]\n",
    "        node_v_emb = embedding_look_up[edge[1]]\n",
    "        feature_vector = np.append(node_u_emb, node_v_emb)\n",
    "        X_test.append(feature_vector)\n",
    "        y_test.append(0)\n",
    "\n",
    "    X = np.concatenate((X_train, X_test))\n",
    "    y = np.concatenate((y_train, y_test))\n",
    "    c = list(zip(X, y))\n",
    "    random.shuffle(c)\n",
    "    X, y = zip(*c)\n",
    "    folds = 10\n",
    "    X_folds = np.array_split(X, folds)\n",
    "    y_folds = np.array_split(y, folds)\n",
    "    accsum=[]\n",
    "    presum=[]\n",
    "    specisum=[]\n",
    "    recallsum=[]\n",
    "    f1sum=[]\n",
    "    aucsum=[]\n",
    "    prcsum=[]\n",
    "    mccsum=[]\n",
    "    for i in range(folds):\n",
    "        X_train = np.vstack(X_folds[:i] + X_folds[i + 1:])\n",
    "        X_val = X_folds[i]\n",
    "        y_train = np.hstack(y_folds[:i] + y_folds[i + 1:])\n",
    "        y_val = y_folds[i]\n",
    "        clf = svm.SVC()\n",
    "        clf.fit(X_train, y_train)  # training the svc model\n",
    "        print('Start predicting...')\n",
    "        pred_y= clf.predict(X_val)\n",
    "        auc_test = roc_auc_score(y_val, pred_y)\n",
    "        pr_test = average_precision_score(y_val, pred_y)\n",
    "        mcc=matthews_corrcoef(y_val, pred_y)\n",
    "        test_num=len(y_val)\n",
    "        tp, fp, tn, fn, accuracy, precision, sensitivity, recall, specificity, MCC, f1_score, Q9, ppv, npv=calculate_performace(test_num, pred_y, y_val)\n",
    "        accsum.append(accuracy)\n",
    "        presum.append(precision)\n",
    "        specisum.append(specificity)\n",
    "        recallsum.append(recall)\n",
    "        f1sum.append(f1_score)\n",
    "        aucsum.append(auc_test)\n",
    "        prcsum.append(pr_test)\n",
    "        mccsum.append(mcc)\n",
    "        print(\"fold：%s accuracy：%s precision：%s\" % (i, accuracy, precision))\n",
    "    with open('result'+str(seed)+'.csv','w') as f:\n",
    "        f.write('acc')\n",
    "        f.write('\\n')\n",
    "        f.write(str(np.mean(accsum)))\n",
    "        f.write('\\n')\n",
    "        f.write(str(np.std(accsum)))\n",
    "        f.write('\\n')\n",
    "        f.write('pre')\n",
    "        f.write('\\n')\n",
    "        f.write(str(np.mean(presum)))\n",
    "        f.write('\\n')\n",
    "        f.write(str(np.std(presum)))\n",
    "        f.write('\\n')\n",
    "        f.write('recall')\n",
    "        f.write('\\n')\n",
    "        f.write(str(np.mean(recallsum)))\n",
    "        f.write('\\n')\n",
    "        f.write(str(np.std(recallsum)))\n",
    "        f.write('\\n')\n",
    "        f.write('speci')\n",
    "        f.write('\\n')\n",
    "        f.write(str(np.mean(specisum)))\n",
    "        f.write('\\n')\n",
    "        f.write(str(np.std(specisum)))\n",
    "        f.write('\\n')\n",
    "        f.write('f1')\n",
    "        f.write('\\n')\n",
    "        f.write(str(np.mean(f1sum)))\n",
    "        f.write('\\n')\n",
    "        f.write(str(np.std(f1sum)))\n",
    "        f.write('\\n')\n",
    "        f.write('auc')\n",
    "        f.write('\\n')\n",
    "        f.write(str(np.mean(accsum)))\n",
    "        f.write('\\n')\n",
    "        f.write(str(np.std(accsum)))\n",
    "        f.write('\\n')\n",
    "        f.write('prc')\n",
    "        f.write('\\n')\n",
    "        f.write(str(np.mean(prcsum)))\n",
    "        f.write('\\n')\n",
    "        f.write(str(np.std(prcsum)))\n",
    "        f.write('\\n')\n",
    "        f.write('mcc')\n",
    "        f.write('\\n')\n",
    "        f.write(str(np.mean(mccsum)))\n",
    "        f.write('\\n')\n",
    "        f.write(str(np.std(mccsum)))\n",
    "\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "protein",
   "language": "python",
   "name": "protein"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
