{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load  utils.py\n",
    "import multiprocessing\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "import copy\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "import OpenNE.graph as og\n",
    "import csv\n",
    "from gensim.models import KeyedVectors\n",
    "from OpenNE import  node2vec\n",
    "\n",
    "def embedding_training(args, train_graph_filename):\n",
    "    '''\n",
    "    load features from network.\n",
    "    '''\n",
    "    G = og.Graph()\n",
    "    G.read_edgelist(train_graph_filename, weighted=False)\n",
    "    _embedding_training(args, G_=G)\n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _embedding_training(args, G_=None):\n",
    "    '''\n",
    "    extract features from network.\n",
    "    '''\n",
    "    seed = args.seed\n",
    "    model = node2vec.Node2vec(graph=G_, path_length=args.walk_length,\n",
    "                              num_paths=args.number_walks, dim=args.dimensions,\n",
    "                              workers=args.workers, p=args.p, q=args.q, window=args.window_size)\n",
    "    model.save_embeddings(args.output)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_graph(input_edgelist,fu_edgelist, seed, testing_ratio=0.2):\n",
    "    '''\n",
    "    split train and test data.\n",
    "    '''\n",
    "    G1 = nx.read_edgelist(input_edgelist)\n",
    "    G0 = nx.read_weighted_edgelist(fu_edgelist)\n",
    "    node_num1, edge_num1 = len(G1.nodes), len(G1.edges)\n",
    "    G_train = copy.deepcopy(G1)\n",
    "    # remove isolate nodes\n",
    "    G_train.remove_nodes_from(list(nx.isolates(G_train)))\n",
    "    # #assert node_num1 == node_num2\n",
    "    train_graph_filename = 'graph_train.edgelist'\n",
    "    nx.write_edgelist(G_train, train_graph_filename, data=False)\n",
    "    node_num1, edge_num1 = len(G_train.nodes), len(G_train.edges)\n",
    "    L1 = list(G_train.nodes())\n",
    "    L0 = list(G0.nodes())\n",
    "    for i in range(len(L0)):\n",
    "        if L0[i] in L1:\n",
    "            continue\n",
    "        else:\n",
    "            G0.remove_node(L0[i])\n",
    "    G0.remove_nodes_from(list(nx.isolates(G0)))\n",
    "    node_num0, edge_num0 = len(G0.nodes), len(G0.edges)\n",
    "    # The number of negative examples to build\n",
    "    num = edge_num1-edge_num0\n",
    "    G = nx.Graph()\n",
    "    threshold = 10\n",
    "    #construct fully connected graph\n",
    "    G.add_edges_from(itertools.combinations(L1, 2))\n",
    "    #remove positive cases\n",
    "    G.remove_edges_from(G_train.edges())\n",
    "    G.remove_edges_from(G0.edges())\n",
    "    random.seed(seed)\n",
    "    for edge in G.edges:\n",
    "        node_u, node_v = edge\n",
    "        if (G.degree(node_u) > threshold and G.degree(node_v) > threshold):\n",
    "            G.remove_edge(node_u, node_v)\n",
    "    print(len(G.edges))\n",
    "    neg_edges = random.sample(G.edges, num)\n",
    "    G0.add_edges_from(neg_edges)\n",
    "    node_num0, edge_num0 = len(G0.nodes), len(G0.edges)\n",
    "    print('dealt neg Graph: nodes:', node_num0, 'edges:', edge_num0)\n",
    "    testing_edges_num = int(len(G_train.edges) * testing_ratio)\n",
    "    random.seed(seed)\n",
    "    testing_pos_edges = random.sample(G_train.edges, testing_edges_num)\n",
    "    G_aux = copy.deepcopy(G_train)\n",
    "    G_aux.remove_edges_from(testing_pos_edges)\n",
    "    train_pos_edges=G_aux.edges()\n",
    "    return G1, G_train, testing_pos_edges, train_graph_filename,G0,train_pos_edges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_neg_edges(G0, edges_num, seed):\n",
    "    '''\n",
    "    generate negative edges\n",
    "    '''\n",
    "    random.seed(seed)\n",
    "    neg_edges = random.sample(G0.edges, edges_num)\n",
    "    return neg_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getKmers(sequence, size):\n",
    "    '''\n",
    "    seperate protein sequence\n",
    "    '''\n",
    "    return [sequence[x:x + size] for x in range(len(sequence) - size + 1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding(embedding_file_name,species):\n",
    "    '''\n",
    "    Generate feature vectors for each protein\n",
    "    '''\n",
    "    dict_club = {}\n",
    "    f = open('seperate.txt', 'w')\n",
    "    with open('data/'+species+'/'+species+'.csv', 'r') as csvfile:  #\n",
    "        reader = csv.reader(csvfile)\n",
    "        for row in reader:\n",
    "            s = getKmers(str(row), 3)\n",
    "            try:\n",
    "                del (s[0])\n",
    "                del (s[0])\n",
    "                del (s[-1])\n",
    "                del (s[-1])\n",
    "                f.write(str(s))\n",
    "                f.write('\\r\\n')\n",
    "            except:\n",
    "                continue\n",
    "    f.close()\n",
    "    sentences = LineSentence(f)\n",
    "    model = Word2Vec(sentences, vector_size=100, window=5, min_count=1 , sg=1 , workers=multiprocessing.cpu_count()-1 )\n",
    "    with open('data/'+species+'/'+species+'.csv','r',encoding='utf-8')as f:\n",
    "        reader = csv.reader(f, delimiter=',')\n",
    "        for row in reader:\n",
    "            dict_club[row[0]] = row[1:]\n",
    "    seqfeature = {}\n",
    "    with open('data/'+species+'/'+species+'.csv', 'r',encoding='utf-8-sig') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for row in reader:\n",
    "            row = ','.join(str(i) for i in row)\n",
    "            arr1 = np.zeros(100)\n",
    "            for x in range(int(len(row) / 3)):\n",
    "                list1 = model[str(row[x * 3:x * 3 + 3])]\n",
    "                arr1 += list1\n",
    "                seqfeature[row] = arr1 / (int(len(row) / 3))\n",
    "    with open(embedding_file_name) as f:\n",
    "        node_num, emb_size = f.readline().split()\n",
    "        print('Nodes with embedding: %s'%node_num)\n",
    "        embedding_look_up = {}\n",
    "        for line in f:\n",
    "            vec = line.strip().split()\n",
    "            node_id = vec[0]\n",
    "            embeddings = vec[1:]\n",
    "            seq1 = dict_club[node_id]\n",
    "            seq1=' '.join(seq1)\n",
    "            emb = [float(x) for x in embeddings]\n",
    "            emb=np.append(emb,seqfeature[seq1])\n",
    "            emb = emb / np.linalg.norm(emb)\n",
    "            emb[np.isnan(emb)] = 0\n",
    "            embedding_look_up[node_id] = list(emb)\n",
    "        assert int(node_num) == len(embedding_look_up)\n",
    "        f.close()\n",
    "        return embedding_look_up\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
